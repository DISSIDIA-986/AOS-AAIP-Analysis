"""
åœ¨çº¿å­¦ä¹ æ›´æ–°æœºåˆ¶
---------------
ç›®çš„ï¼šéšç€æ–°æŠ½ç­¾æ•°æ®äº§ç”Ÿï¼Œå¢é‡æ›´æ–°æ¨¡å‹è€Œéå®Œå…¨é‡æ–°è®­ç»ƒã€‚
é€‚ç”¨åœºæ™¯ï¼š2025å¹´12æœˆæ•°æ®é™†ç»­äº§ç”Ÿï¼Œéœ€å®æ—¶æ•æ‰æœ€æ–°è¶‹åŠ¿ã€‚

æ–¹æ³•ï¼š
1. æ»‘åŠ¨çª—å£ï¼šä¿ç•™æœ€è¿‘Næ¬¡æŠ½ç­¾ä½œä¸ºè®­ç»ƒé›†
2. å¢é‡æ›´æ–°ï¼šæ·»åŠ æ–°æ•°æ®åé‡æ–°è®­ç»ƒï¼ˆæ ‘æ¨¡å‹éœ€å®Œå…¨é‡è®­ï¼Œä½†æ•°æ®çª—å£é™åˆ¶äº†è®¡ç®—é‡ï¼‰
3. æ€§èƒ½ç›‘æ§ï¼šè·Ÿè¸ªæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„MAEï¼Œæ£€æµ‹æ€§èƒ½é€€åŒ–
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error

DATA_PATH = Path("data/processed/aaip_draws_2025.csv")
MODEL_REGISTRY = Path("models/model_registry.json")
PERFORMANCE_LOG = Path("models/performance_log.csv")


def load_model_registry() -> Dict[str, Dict]:
    """åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨ï¼ˆå­˜å‚¨æ¯ä¸ªæµçš„æœ€ä¼˜æ¨¡å‹é…ç½®ï¼‰ã€‚"""
    if MODEL_REGISTRY.exists():
        with open(MODEL_REGISTRY, "r") as f:
            return json.load(f)
    return {}


def save_model_registry(registry: Dict[str, Dict]) -> None:
    """ä¿å­˜æ¨¡å‹æ³¨å†Œè¡¨ã€‚"""
    MODEL_REGISTRY.parent.mkdir(parents=True, exist_ok=True)
    with open(MODEL_REGISTRY, "w") as f:
        json.dump(registry, f, indent=2)


def incremental_update(
    stream: str,
    new_data: pd.DataFrame,
    window_size: int = 50,
    feature_cols: List[str] = None,
) -> Dict[str, object]:
    """
    å¢é‡æ›´æ–°å•ä¸ªæµçš„æ¨¡å‹ã€‚

    Args:
        stream: æµåç§°
        new_data: æ–°å¢çš„æŠ½ç­¾æ•°æ®
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆä¿ç•™æœ€è¿‘Næ¬¡æŠ½ç­¾ï¼‰
        feature_cols: ç‰¹å¾åˆ—è¡¨

    Returns:
        æ›´æ–°ç»“æœå­—å…¸ï¼ˆåŒ…å«æ–°MAEã€æ•°æ®é‡ç­‰ï¼‰
    """
    if feature_cols is None:
        feature_cols = [
            "date_ord", "gap_days", "lag1_inv", "lag2_inv", "lag3_inv",
            "roll3_inv", "roll5_inv", "lag1_score", "sin_doy", "cos_doy",
            "month_num", "event_index", "is_holiday_week", "is_priority_sector",
            "cumulative_invitations", "gap_deviation", "is_gap_anomaly"
        ]

    # åŠ è½½æ³¨å†Œè¡¨
    registry = load_model_registry()
    stream_config = registry.get(stream, {"model_type": "RandomForest"})

    # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    stream_data = new_data[new_data["stream"] == stream].dropna(subset=["invitations"]).sort_values("draw_date")

    if len(stream_data) < 15:
        return {
            "stream": stream,
            "status": "insufficient_data",
            "data_points": len(stream_data),
        }

    # å–æœ€è¿‘window_sizeæ¡è®°å½•
    train_data = stream_data.tail(window_size).copy()
    train_data = train_data.dropna(subset=feature_cols + ["invitations"])

    if train_data.empty or len(train_data) < 10:
        return {
            "stream": stream,
            "status": "insufficient_valid_data",
            "data_points": len(train_data),
        }

    # è®­ç»ƒæµ‹è¯•åˆ†å‰²ï¼ˆæœ€å3æ¬¡ä½œä¸ºéªŒè¯ï¼‰
    if len(train_data) < 13:
        return {
            "stream": stream,
            "status": "insufficient_for_validation",
            "data_points": len(train_data),
        }

    train, test = train_data.iloc[:-3], train_data.iloc[-3:]

    # è®­ç»ƒæ¨¡å‹
    if stream_config["model_type"] == "RandomForest":
        model = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42)
    elif stream_config["model_type"] == "GradientBoosting":
        model = GradientBoostingRegressor(n_estimators=150, random_state=42)
    else:
        model = RandomForestRegressor(n_estimators=300, random_state=42)

    model.fit(train[feature_cols], train["invitations"])

    # è¯„ä¼°
    predictions = model.predict(test[feature_cols])
    mae = mean_absolute_error(test["invitations"], predictions)

    # æ›´æ–°æ³¨å†Œè¡¨
    stream_config.update({
        "model_type": stream_config["model_type"],
        "last_updated": datetime.now().isoformat(),
        "training_samples": len(train),
        "validation_mae": round(float(mae), 2),
        "window_size": window_size,
    })
    registry[stream] = stream_config
    save_model_registry(registry)

    # è®°å½•æ€§èƒ½æ—¥å¿—
    log_performance(stream, mae, len(train), len(test))

    return {
        "stream": stream,
        "status": "updated",
        "mae": round(float(mae), 2),
        "training_samples": len(train),
        "test_samples": len(test),
        "model_type": stream_config["model_type"],
    }


def log_performance(stream: str, mae: float, train_size: int, test_size: int) -> None:
    """è®°å½•æ¨¡å‹æ€§èƒ½åˆ°æ—¥å¿—æ–‡ä»¶ã€‚"""
    PERFORMANCE_LOG.parent.mkdir(parents=True, exist_ok=True)

    log_entry = pd.DataFrame([{
        "timestamp": datetime.now().isoformat(),
        "stream": stream,
        "mae": mae,
        "train_size": train_size,
        "test_size": test_size,
    }])

    if PERFORMANCE_LOG.exists():
        existing = pd.read_csv(PERFORMANCE_LOG)
        log_entry = pd.concat([existing, log_entry], ignore_index=True)

    log_entry.to_csv(PERFORMANCE_LOG, index=False)


def detect_performance_degradation(stream: str, threshold: float = 1.2) -> Dict[str, object]:
    """
    æ£€æµ‹æ¨¡å‹æ€§èƒ½é€€åŒ–ã€‚

    Args:
        stream: æµåç§°
        threshold: é€€åŒ–é˜ˆå€¼ï¼ˆå½“å‰MAE / å†å²æœ€ä½³MAEï¼‰

    Returns:
        é€€åŒ–æ£€æµ‹ç»“æœ
    """
    if not PERFORMANCE_LOG.exists():
        return {"stream": stream, "degradation_detected": False, "reason": "no_history"}

    log = pd.read_csv(PERFORMANCE_LOG)
    stream_log = log[log["stream"] == stream]

    if len(stream_log) < 2:
        return {"stream": stream, "degradation_detected": False, "reason": "insufficient_history"}

    historical_best = stream_log["mae"].min()
    current_mae = stream_log.iloc[-1]["mae"]

    ratio = current_mae / historical_best

    return {
        "stream": stream,
        "degradation_detected": ratio > threshold,
        "current_mae": round(float(current_mae), 2),
        "historical_best_mae": round(float(historical_best), 2),
        "degradation_ratio": round(float(ratio), 2),
        "threshold": threshold,
    }


def batch_update_all_streams(data_path: Path = DATA_PATH) -> List[Dict[str, object]]:
    """æ‰¹é‡æ›´æ–°æ‰€æœ‰æµçš„æ¨¡å‹ã€‚"""
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from src.enhanced_modeling import add_enhanced_features

    df = pd.read_csv(data_path, parse_dates=["draw_date"])
    feats = add_enhanced_features(df)

    results = []
    for stream in feats["stream"].unique():
        result = incremental_update(stream, feats)
        results.append(result)

        # æ£€æµ‹é€€åŒ–
        degradation = detect_performance_degradation(stream)
        if degradation["degradation_detected"]:
            print(f"âš ï¸  {stream}: æ€§èƒ½é€€åŒ–æ£€æµ‹ï¼MAEä»{degradation['historical_best_mae']}å‡è‡³{degradation['current_mae']}")

    return results


def generate_update_report(results: List[Dict[str, object]], output_path: Path) -> None:
    """ç”Ÿæˆæ›´æ–°æŠ¥å‘Šã€‚"""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("# åœ¨çº¿å­¦ä¹ æ›´æ–°æŠ¥å‘Š\n\n")
        f.write(f"**æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## æ›´æ–°ç»“æœ\n\n")
        f.write("| Stream | Status | MAE | Training Samples | Model Type |\n")
        f.write("| --- | --- | --- | --- | --- |\n")

        for res in results:
            stream = res.get("stream", "Unknown")
            status = res.get("status", "unknown")
            mae = res.get("mae", "N/A")
            train_samples = res.get("training_samples", "N/A")
            model_type = res.get("model_type", "N/A")

            status_icon = "âœ…" if status == "updated" else "âš ï¸"
            f.write(f"| {stream} | {status_icon} {status} | {mae} | {train_samples} | {model_type} |\n")

        f.write("\n## æ€§èƒ½é€€åŒ–æ£€æµ‹\n\n")
        for res in results:
            stream = res.get("stream")
            degradation = detect_performance_degradation(stream)

            if degradation.get("degradation_detected"):
                f.write(f"### âš ï¸ {stream}\n\n")
                f.write(f"- å½“å‰MAE: {degradation['current_mae']}\n")
                f.write(f"- å†å²æœ€ä½³: {degradation['historical_best_mae']}\n")
                f.write(f"- é€€åŒ–æ¯”ä¾‹: {degradation['degradation_ratio']}x\n")
                f.write(f"- **å»ºè®®**: æ£€æŸ¥æœ€æ–°æ•°æ®åˆ†å¸ƒï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç‰¹å¾æˆ–æ¨¡å‹ã€‚\n\n")

        if not any(detect_performance_degradation(r["stream"]).get("degradation_detected") for r in results):
            f.write("âœ… æ‰€æœ‰æµæ€§èƒ½ç¨³å®šï¼Œæœªæ£€æµ‹åˆ°æ˜¾è‘—é€€åŒ–ã€‚\n\n")

        f.write("---\n\n")
        f.write("**ä¸‹æ¬¡æ›´æ–°**: å»ºè®®æ¯å‘¨æˆ–æ–°å¢5+æ¬¡æŠ½ç­¾åé‡æ–°è¿è¡Œã€‚\n")


def run() -> None:
    """æ‰§è¡Œåœ¨çº¿å­¦ä¹ æ›´æ–°ã€‚"""
    print("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°æ‰€æœ‰æµçš„æ¨¡å‹...")
    results = batch_update_all_streams()

    print("\nğŸ“Š æ›´æ–°ç»“æœï¼š")
    for res in results:
        status_icon = "âœ…" if res.get("status") == "updated" else "âš ï¸"
        print(f"  {status_icon} {res['stream']}: {res.get('status')} (MAE: {res.get('mae', 'N/A')})")

    report_path = Path("reports/online_learning_update.md")
    generate_update_report(results, report_path)
    print(f"\nâœ… æ›´æ–°æŠ¥å‘Šå·²ç”Ÿæˆ -> {report_path}")


if __name__ == "__main__":
    run()
